# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DkWZYSlTY84HH1GtdKF2TRQtusMZsb5p
"""

import pandas as pd

# Load the crawl data from a CSV file
crawl_data = pd.read_csv('crawl_data.csv')

# Display the first few rows of the data to understand its structure
print(crawl_data.head())

# Extract the domain or category from the URLs (assuming the category is a part of the URL structure)
def extract_category(url):
    # Modify this function to match your URL structure
    parts = url.split('/')
    if len(parts) > 3:
        return parts[3]  # Example: extracting the 4th part as the category
    return 'unknown'

# Add a new column for the categories of the source and target URLs
crawl_data['source_category'] = crawl_data['source_url'].apply(extract_category)
crawl_data['target_category'] = crawl_data['target_url'].apply(extract_category)

# Group by source_category and target_category to count internal links
internal_link_counts = crawl_data.groupby(['source_category', 'target_category']).size().reset_index(name='link_count')

# Pivot the table to see the count of internal links to each category
pivot_table = internal_link_counts.pivot(index='source_category', columns='target_category', values='link_count').fillna(0)

# Save the pivot table to a CSV file for further analysis
pivot_table.to_csv('internal_link_analysis.csv')

# Display the pivot table
print(pivot_table)